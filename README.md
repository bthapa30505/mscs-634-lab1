# MSCS-634-lab1

The purpose of this lab was to learn data visualization, data preprocessing, and statistical analysis using Python, So this lab work focuses on comprehensive data preprocessing and statistical analysis of a mobile phone dataset containing specifications and pricing information. The primary objectives were to clean and prepare raw data by handling missing values, detecting and removing outliers, and reducing data dimensionality; apply data transformation techniques including scaling and discretization to make data suitable for machine learning applications; and perform in-depth statistical analysis to understand data characteristics, distributions, and relationships between variables that can inform business decisions regarding mobile phone pricing, features, and market segmentation.

The preprocessing phase revealed significant data quality issues that required careful handling. Missing values were primarily found in camera specifications and extended memory columns, which were addressed using median imputation for numerical features and mode for categorical features to maintain data integrity. Outlier detection using the IQR method identified approximately 15-25% of records as extreme values, particularly in price and camera specifications, representing ultra-premium flagship devices that could skew statistical models. Data reduction through 70% sampling and elimination of low-correlation features (|r| < 0.1 with price) successfully reduced dataset size while preserving 95%+ of information value.

Statistical analysis uncovered important market dynamics in the mobile phone industry. The price distribution showed a right-skewed pattern.Also, Correlation analysis identified camera quality and display refresh rate as primary price drivers (r â‰ˆ 0.65-0.75 and 0.55-0.65 respectively), while battery capacity and IR blaster features showed weak correlations with price, suggesting these features don't command significant premiums despite their utility.

I faced several key challenges during the lab work that required thoughtful decision-making. These ranged from technical issues with jupyter notebook setup to specific issues with the dataset cleanup and processing. One of the crucial challenge was balancing outlier removal with preserving valuable information about the premium segment. I ultimately chose the standard IQR method with 1.5 threshold, accepting 15-25% data loss to improve model reliability while documenting outlier characteristics separately for business insights.
